import org.apache.hadoop.fs.Path;
import org.apache.hadoop.conf.*;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
public class drv{
@SuppressWarnings("deprecation")
public static void main(String[] args)throws Exception
{
 Configuration conf=new Configuration();
 Job job=new Job(conf,"miniproject");
job.setJarByClass(drv.class);
FileInputFormat.addInputPath(job, new Path(args[0]));
Path outputPath =new Path(args[1]);
FileOutputFormat.setOutputPath(job, outputPath);
 outputPath.getFileSystem(conf).delete(outputPath, true);
 
 job.setMapperClass(mpr.class);
 job.setReducerClass(rdcr.class);
 
 job.setInputFormatClass(TextInputFormat.class);
 job.setOutputFormatClass(TextOutputFormat.class);
 
 job.setMapOutputKeyClass(Text.class);
 job.setMapOutputValueClass(IntWritable.class);
 
 job.setOutputKeyClass(Text.class);
 job.setOutputValueClass(IntWritable.class);
 
job.waitForCompletion(true);
}
}



MAPPER


 import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.*;

public class mpr extends Mapper<LongWritable, Text,Text,IntWritable>{
Text outkey=new Text();
IntWritable outvalue=new IntWritable();
public void map(LongWritable key, Text value, Context context)
 throws IOException, InterruptedException{

    String[] lineArray = value.toString().split(",");
 if(lineArray.length>15)
 {
     if(lineArray[8].equalsIgnoreCase("true"))
     {
    if(lineArray[5].matches(".*(THEFT).*"))
    {
        outkey.set(lineArray[11]);
        outvalue.set(1);
       context.write(outkey,outvalue);
    }
     }
 }
 

}

}





REDUCER



import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.*;
import org.apache.hadoop.mapreduce.Mapper.Context;
public class rdcr extends Reducer<Text,IntWritable,Text,IntWritable>{
 public void reduce(Text key, Iterable<IntWritable>values, Context context)
 throws IOException, InterruptedException{
 int sum=0;
for(IntWritable value: values)
{ sum=sum+value.get();
}
 context.write(key, new IntWritable(sum)); }
}





PIG

a = load '/home/acadgild/mapreduce/Crimesdata.txt' using PigStorage(',') as (id:chararray ,cn:chararray, c:chararray ,d:chararray ,e:chararray ,f:chararray ,g:chararray ,h:chararray ,i:chararray ,j:chararray ,k:chararray ,l:chararray ,m:chararray ,n:chararray ,o:chararray ,p:chararray ,q:chararray ,r:chararray ,s:chararray ,t:chararray ,u:chararray ,v:chararray);

b = filter a by i == 'true';

 c = FILTER b BY (f matches '.*THEFT.*');

 d = group c by l;

 e = foreach d generate group,COUNT(c.id);

dump e;
